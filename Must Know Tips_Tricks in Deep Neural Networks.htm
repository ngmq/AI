<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/">
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" href="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/jemdoc.css" type="text/css">
<title>Must Know Tips/Tricks in Deep Neural Networks (by &lt;a href="http://lamda.nju.edu.cn/weixs/"&gt;Xiu-Shen Wei&lt;/a&gt;)</title>
</head>
<body>
<div id="layout-content">
<div id="toptitle">
<h1>Must Know Tips/Tricks in Deep Neural Networks (by <a href="http://lamda.nju.edu.cn/weixs/">Xiu-Shen Wei</a>)</h1>
</div>
<table class="imgtable"><tbody><tr><td>
<img src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/tricks.JPG" alt="tricks" height="235px" width="189px">&nbsp;</td>
<td align="left"><p>Deep Neural Networks, especially <b><i>Convolutional Neural Networks</i></b> (<i>CNN</i>),
 allows computational models that are composed of multiple processing 
layers to learn representations of data with multiple levels of 
abstraction. These methods have dramatically improved the 
state-of-the-arts in visual object recognition, object detection, text 
recognition and many other domains such as drug discovery and genomics.</p>
<p>In addition, many solid papers have been published in this topic, and
 some high quality open source CNN software packages have been made 
available. There are also well-written CNN tutorials or CNN software 
manuals. However, it might lack a recent and comprehensive summary about
 the details of how to implement an excellent deep convolutional neural 
networks from scratch. Thus, we collected and concluded many 
implementation details for DCNNs. <b>Here we will introduce these extensive implementation details, i.e., <i>tricks</i> or <i>tips</i>, for building and training your own deep networks.</b></p>
</td></tr></tbody></table>
<h2>Introduction</h2>
<p>We assume you already know the basic knowledge of deep learning, and 
here we will present the implementation details (tricks or tips) in Deep
 Neural Networks, especially CNN for image-related tasks, mainly in <b>eight aspects</b>: <b>1)</b> <i>data augmentation</i>; <b>2)</b> <i>pre-processing on images</i>; <b>3)</b> <i>initializations of Networks</i>; <b>4)</b> <i>some tips during training</i>; <b>5)</b> <i>selections of activation functions</i>; <b>6)</b> <i>diverse regularizations</i>; <b>7)</b> <i>some insights found from figures</i> and finally <b>8)</b> <i>methods of ensemble multiple deep networks</i>.</p>
<p>Additionally, the <b>corresponding slides</b> are available at <b><tt><a href="http://lamda.nju.edu.cn/weixs/slide/CNNTricks_slide.pdf">[slide]</a></tt></b>.
 If there are any problems/mistakes in these materials and slides, or 
there are something important/interesting you consider that should be 
added, just feel free to contact <a href="http://lamda.nju.edu.cn/weixs/">me</a>.</p>
<h2>Sec. 1: Data Augmentation</h2>
<p>Since deep networks need to be trained on a huge number of training 
images to achieve satisfactory performance, if the original image data 
set contains limited training images, it is better to do data 
augmentation to boost the performance. Also, data augmentation becomes 
the thing must to do when training a deep network.</p>
<ul>
<li><p>There are many ways to do data augmentation, such as the popular <b>horizontally flipping</b>, <b>random crops</b> and <b>color jittering</b>.
 Moreover, you could try combinations of multiple different processing, 
e.g., doing the rotation and random scaling at the same time. In 
addition, you can try to raise saturation and value (S and V components 
of the HSV color space) of all pixels to a power between 0.25 and 4 
(same for all pixels within a patch), multiply these values by a factor 
between 0.7 and 1.4, and add to them a value between -0.1 and 0.1. Also,
 you could add a value between [-0.1, 0.1] to the hue (H component of 
HSV) of all pixels in the image/patch.</p>
</li>
</ul>
<ul>
<li><p>Krizhevsky <i>et al</i>. <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">[1]</a> proposed <b><i>fancy PCA</i></b> when training the famous <i>Alex-Net</i>
 in 2012. Fancy PCA alters the intensities of the RGB channels in 
training images. In practice, you can firstly perform PCA on the set of 
RGB pixel values throughout your training images. And then, for each 
training image, just add the following quantity to each RGB image pixel 
(i.e., <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/1689103090430267023-130.png" alt="I_{xy}=[I_{xy}^R,I_{xy}^G,I_{xy}^B]^T" style="vertical-align: -8px">): <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/7646882281153414176-130.png" alt="[bf{p}_1,bf{p}_2,bf{p}_3][alpha_1 lambda_1,alpha_2 lambda_2,alpha_3 lambda_3]^T" style="vertical-align: -5px"> where, <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/4533528109326269078-130.png" alt="bf{p}_i" style="vertical-align: -4px"> and <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/2768874965527882936-130.png" alt="lambda_i" style="vertical-align: -4px"> are the <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/13440040424-130.png" alt="i" style="vertical-align: -1px">-th eigenvector and eigenvalue of the <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/2577508771177243569-130.png" alt="3times 3" style="vertical-align: -1px"> covariance matrix of RGB pixel values, respectively, and <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/2031553203773749404-130.png" alt="alpha_i" style="vertical-align: -4px"> is a random variable drawn from a Gaussian with mean zero and standard deviation 0.1. Please note that, each <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/2031553203773749404-130.png" alt="alpha_i" style="vertical-align: -4px">
 is drawn only once for all the pixels of a particular training image 
until that image is used for training again. That is to say, when the 
model meets the same training image again, it will randomly produce 
another <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/2031553203773749404-130.png" alt="alpha_i" style="vertical-align: -4px"> for data augmentation. In <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">[1]</a>, they claimed that “<i>fancy
 PCA could approximately capture an important property of natural 
images, namely, that object identity is invariant to changes in the 
intensity and color of the illumination</i>”. To the classification performance, this scheme reduced the top-1 error rate by over 1% in the competition of ImageNet 2012.</p>
</li>
</ul>
<h2>Sec. 2: Pre-Processing</h2>
<p>Now we have obtained a large number of training samples 
(images/crops), but please do not hurry! Actually, it is necessary to do
 pre-processing on these images/crops. In this section, we will 
introduce several approaches for pre-processing.</p>
<p>The first and simple pre-processing approach is <b>zero-center</b> the data, and then <b>normalize</b> them, which is presented as two lines Python codes as follows:</p>
<div class="codeblock">
<div class="blockcontent"><pre><span class="pycommand">&gt;&gt;&gt; X -= np.mean(X, axis = 0) <span class="comment"># zero-center</span></span>
<span class="pycommand">&gt;&gt;&gt; X /= np.std(X, axis = 0) <span class="comment"># normalize</span></span>
</pre></div></div>
<p>where, X is the input data (NumIns×NumDim). Another form of this 
pre-processing normalizes each dimension so that the min and max along 
the dimension is -1 and 1 respectively. It only makes sense to apply 
this pre-processing if you have a reason to believe that different input
 features have different scales (or units), but they should be of 
approximately equal importance to the learning algorithm. In case of 
images, the relative scales of pixels are already approximately equal 
(and in range from 0 to 255), so it is not strictly necessary to perform
 this additional pre-processing step.</p>
<p>Another pre-processing approach similar to the first one is <b>PCA Whitening</b>.
 In this process, the data is first centered as described above. Then, 
you can compute the covariance matrix that tells us about the 
correlation structure in the data:</p>
<div class="codeblock">
<div class="blockcontent"><pre><span class="pycommand">&gt;&gt;&gt; X -= np.mean(X, axis = 0) <span class="comment"># zero-center</span></span>
<span class="pycommand">&gt;&gt;&gt; cov = np.dot(X.T, X) / X.shape[0] <span class="comment"># compute the covariance matrix</span></span>
</pre></div></div>
<p>After that, you decorrelate the data by projecting the original (but zero-centered) data into the eigenbasis:</p>
<div class="codeblock">
<div class="blockcontent"><pre><span class="pycommand">&gt;&gt;&gt; U,S,V = np.linalg.svd(cov) <span class="comment"># compute the SVD factorization of the data covariance matrix</span></span>
<span class="pycommand">&gt;&gt;&gt; Xrot = np.dot(X, U) <span class="comment"># decorrelate the data</span></span>
</pre></div></div>
<p>The last transformation is whitening, which takes the data in the 
eigenbasis and divides every dimension by the eigenvalue to normalize 
the scale:</p>
<div class="codeblock">
<div class="blockcontent"><pre><span class="pycommand">&gt;&gt;&gt; Xwhite = Xrot / np.sqrt(S + 1e-5) <span class="comment"># divide by the eigenvalues (which are square roots of the singular values)</span></span>
</pre></div></div>
<p>Note that here it adds 1e-5 (or a small constant) to prevent division
 by zero. One weakness of this transformation is that it can greatly 
exaggerate the noise in the data, since it stretches all dimensions 
(including the irrelevant dimensions of tiny variance that are mostly 
noise) to be of equal size in the input. This can in practice be 
mitigated by stronger smoothing (i.e., increasing 1e-5 to be a larger 
number).</p>
<p>Please note that, we describe these pre-processing here just for 
completeness. In practice, these transformations are not used with 
Convolutional Neural Networks. However, it is also very important to <b>zero-center</b> the data, and it is common to see <b>normalization</b> of every pixel as well.</p>
<h2>Sec. 3: Initializations</h2>
<p>Now the data is ready. However, before you are beginning to train the network, you have to initialize its parameters.</p>
<h3>All Zero Initialization</h3>
<p>In the ideal situation, with proper data normalization it is 
reasonable to assume that approximately half of the weights will be 
positive and half of them will be negative. A reasonable-sounding idea 
then might be to set <i>all the initial weights to zero</i>, which you 
expect to be the “best guess” in expectation. But, this turns out to be a
 mistake, because if every neuron in the network computes the same 
output, then they will also all compute the same gradients during 
back-propagation and undergo the exact same parameter updates. In other 
words, there is no source of asymmetry between neurons if their weights 
are initialized to be the same.</p>
<h3>Initialization with Small Random Numbers</h3>
<p>Thus, you still want the weights to be very close to zero, but not 
identically zero. In this way, you can random these neurons to small 
numbers which are very close to zero, and it is treated as <i>symmetry breaking</i>.
 The idea is that the neurons are all random and unique in the 
beginning, so they will compute distinct updates and integrate 
themselves as diverse parts of the full network. The implementation for 
weights might simply look like <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/7565991700483225477-130.png" alt="weightssim 0.001times N(0,1)" style="vertical-align: -5px">, where <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/8770156396093607298-130.png" alt="N(0,1)" style="vertical-align: -5px">
 is a zero mean, unit standard deviation gaussian. It is also possible 
to use small numbers drawn from a uniform distribution, but this seems 
to have relatively little impact on the final performance in practice.</p>
<h3>Calibrating the Variances</h3>
<p>One problem with the above suggestion is that the distribution of the
 outputs from a randomly initialized neuron has a variance that grows 
with the number of inputs. It turns out that you can normalize the 
variance of each neuron's output to 1 by scaling its weight vector by 
the square root of its <i>fan-in</i> (i.e., its number of inputs), which is as follows:</p>
<div class="codeblock">
<div class="blockcontent"><pre><span class="pycommand">&gt;&gt;&gt; w = np.random.randn(n) / sqrt(n) <span class="comment"># calibrating the variances with 1/sqrt(n)</span></span>
</pre></div></div>
<p>where “randn” is the aforementioned Gaussian and “n” is the number of
 its inputs. This ensures that all neurons in the network initially have
 approximately the same output distribution and empirically improves the
 rate of convergence. The detailed derivations can be found from Page. 
18 to 23 of the slides. Please note that, in the derivations, it does 
not consider the influence of ReLU neurons.</p>
<h3>Current Recommendation</h3>
<p>As aforementioned, the previous initialization by calibrating the 
variances of neurons is without considering ReLUs. A more recent paper 
on this topic by He <i>et al</i>. <a href="http://arxiv.org/abs/1502.01852">[4]</a>
 derives an initialization specifically for ReLUs, reaching the 
conclusion that the variance of neurons in the network should be <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/4909253699508052766-130.png" alt="2.0/n" style="vertical-align: -5px"> as:</p>
<div class="codeblock">
<div class="blockcontent"><pre><span class="pycommand">&gt;&gt;&gt; w = np.random.randn(n) * sqrt(2.0/n) <span class="comment"># current recommendation</span></span>
</pre></div></div>
<p>which is the current recommendation for use in practice, as discussed in <a href="http://arxiv.org/abs/1502.01852">[4]</a>.</p>
<h2>Sec. 4: During Training</h2>
<p>Now, everything is ready. Let’s start to train deep networks!</p>
<ul>
<li><p><b>Filters and pooling size</b>. During training, the size of input images prefers to be power-of-2, such as 32 (e.g., <i>CIFAR-10</i>), 64, 224 (e.g., common used <i>ImageNet</i>), 384 or 512, etc. Moreover, it is important to employ a small filter (e.g., <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/2577508771177243569-130.png" alt="3times 3" style="vertical-align: -1px">)
 and small strides (e.g., 1) with zeros-padding, which not only reduces 
the number of parameters, but improves the accuracy rates of the whole 
deep network. Meanwhile, a special case mentioned above, i.e., <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/2577508771177243569-130.png" alt="3times 3" style="vertical-align: -1px">
 filters with stride 1, could preserve the spatial size of 
images/feature maps. For the pooling layers, the common used pooling 
size is of <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/3609640256411903583-130.png" alt="2times 2" style="vertical-align: -0px">.</p>
</li>
</ul>
<ul>
<li><p><b>Learning rate</b>. In addition, as described in a blog by Ilya Sutskever <a href="http://yyue.blogspot.sg/2015/01/a-brief-overview-of-deep-learning.html/">[2]</a>,
 he recommended to divide the gradients by mini batch size. Thus, you 
should not always change the learning rates (LR), if you change the mini
 batch size. For obtaining an appropriate LR, utilizing the validation 
set is an effective way. Usually, a typical value of LR in the beginning
 of your training is 0.1. In practice, if you see that you stopped 
making progress on the validation set, divide the LR by 2 (or by 5), and
 keep going, which might give you a surprise.</p>
</li>
</ul>
<ul>
<li><p><b>Fine-tune on pre-trained models</b>. Nowadays, many state-of-the-arts deep networks are released by famous research groups, i.e., <i><a href="https://github.com/BVLC/caffe/wiki/Model-Zoo">Caffe Model Zoo</a></i> and <i><a href="http://www.vlfeat.org/matconvnet/pretrained/">VGG Group</a></i>.
 Thanks to the wonderful generalization abilities of pre-trained deep 
models, you could employ these pre-trained models for your own 
applications directly. For further improving the classification 
performance on your data set, a very simple yet effective approach is to
 fine-tune the pre-trained models on your own data. As shown in 
following table, the two most important factors are the size of the new 
data set (small or big), and its similarity to the original data set. 
Different strategies of fine-tuning can be utilized in different 
situations. For instance, a good case is that your new data set is very 
similar to the data used for training pre-trained models. In that case, 
if you have very little data, you can just train a linear classifier on 
the features extracted from the top layers of pre-trained models. If 
your have quite a lot of data at hand, please fine-tune a few top layers
 of pre-trained models with a small learning rate. However, if your own 
data set is quite different from the data used in pre-trained models but
 with enough training images, a large number of layers should be 
fine-tuned on your data also with a small learning rate for improving 
performance. However, if your data set not only contains little data, 
but is very different from the data used in pre-trained models, you will
 be in trouble. Since the data is limited, it seems better to only train
 a linear classifier. Since the data set is very different, it might not
 be best to train the classifier from the top of the network, which 
contains more dataset-specific features. Instead, it might work better 
to train the SVM classifier on activations/features from somewhere 
earlier in the network.</p>
</li>
</ul>
<table class="imgtable"><tbody><tr><td>
<img src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/table.png" alt="table" height="215px" width="665px">&nbsp;</td>
<td align="left"><p>Fine-tune your data on pre-trained models. Different
 strategies of fine-tuning are utilized in different situations. For 
data sets, <i>Caltech-101</i> is similar to <i>ImageNet</i>, where both two are object-centric image data sets; while <i>Place Database</i> is different from <i>ImageNet</i>, where one is scene-centric and the other is object-centric.</p>
</td></tr></tbody></table>
<h2>Sec. 5: Activation Functions</h2>
<p>One of the crucial factors in deep networks is <i>activation function</i>, which brings the <b>non-linearity</b>
 into networks. Here we will introduce the details and characters of 
some popular activation functions and give advices later in this 
section.</p>
<table class="imgtable"><tbody><tr><td>
<img src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/neuron.png" alt="neuron" height="215px" width="685px">&nbsp;</td>
<td align="left"><p>Figures courtesy of <a href="http://cs231n.stanford.edu/index.html"><i>Stanford CS231n</i></a>.</p>
</td></tr></tbody></table>
<h3>Sigmoid</h3>
<table class="imgtable"><tbody><tr><td>
<img src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/sigmod.png" alt="sigmod" height="165px" width="225px">&nbsp;</td>
<td align="left"><p>The sigmoid non-linearity has the mathematical form <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/4713673387403833492-130.png" alt="sigma(x)=1/(1+e^{-x})" style="vertical-align: -5px">.
 It takes a real-valued number and “squashes” it into range between 0 
and 1. In particular, large negative numbers become 0 and large positive
 numbers become 1. The sigmoid function has seen frequent use 
historically since it has a nice interpretation as the firing rate of a 
neuron: from not firing at all (0) to fully-saturated firing at an 
assumed maximum frequency (1).</p>
</td></tr></tbody></table>
<p>In practice, the sigmoid non-linearity has recently fallen out of favor and it is rarely ever used. It has two major drawbacks:</p>
<ol>
<li><p><i>Sigmoids saturate and kill gradients</i>. A very undesirable 
property of the sigmoid neuron is that when the neuron's activation 
saturates at either tail of 0 or 1, the gradient at these regions is 
almost zero. Recall that during back-propagation, this (local) gradient 
will be multiplied to the gradient of this gate's output for the whole 
objective. Therefore, if the local gradient is very small, it will 
effectively “kill” the gradient and almost no signal will flow through 
the neuron to its weights and recursively to its data. Additionally, one
 must pay extra caution when initializing the weights of sigmoid neurons
 to prevent saturation. For example, if the initial weights are too 
large then most neurons would become saturated and the network will 
barely learn.</p>
</li>
<li><p><i>Sigmoid outputs are not zero-centered</i>. This is undesirable
 since neurons in later layers of processing in a Neural Network (more 
on this soon) would be receiving data that is not zero-centered. This 
has implications on the dynamics during gradient descent, because if the
 data coming into a neuron is always positive (e.g., <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/5999452984634080335-130.png" alt="x&gt;0" style="vertical-align: -1px"> element wise in <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/2668512978356043857-130.png" alt="f=w^Tx+b" style="vertical-align: -4px">), then the gradient on the weights <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/15232045814-130.png" alt="w" style="vertical-align: -1px"> will during back-propagation become either all be positive, or all negative (depending on the gradient of the whole expression <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/13056039271-130.png" alt="f" style="vertical-align: -4px">).
 This could introduce undesirable zig-zagging dynamics in the gradient 
updates for the weights. However, notice that once these gradients are 
added up across a batch of data the final update for the weights can 
have variable signs, somewhat mitigating this issue. Therefore, this is 
an inconvenience but it has less severe consequences compared to the 
saturated activation problem above.</p>
</li>
</ol>
<h3>tanh(x)</h3>
<table class="imgtable"><tbody><tr><td>
<img src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/tanh.png" alt="tanh" height="175px" width="235px">&nbsp;</td>
<td align="left"><p>The tanh non-linearity squashes a real-valued number
 to the range [-1, 1]. Like the sigmoid neuron, its activations 
saturate, but unlike the sigmoid neuron its output is zero-centered. 
Therefore, in practice the tanh non-linearity is always preferred to the
 sigmoid nonlinearity.</p>
</td></tr></tbody></table>
<h3>Rectified Linear Unit</h3>
<table class="imgtable"><tbody><tr><td>
<img src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/relu.png" alt="relu" height="165px" width="235px">&nbsp;</td>
<td align="left"><p>The Rectified Linear Unit (ReLU) has become very popular in the last few years. It computes the function <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/3442307160410329206-130.png" alt="f(x)=max(0,x)" style="vertical-align: -5px">, which is simply thresholded at zero.</p>
</td></tr></tbody></table>
<p>There are several pros and cons to using the ReLUs:</p>
<ol>
<li><p>(<i>Pros</i>) Compared to sigmoid/tanh neurons that involve 
expensive operations (exponentials, etc.), the ReLU can be implemented 
by simply thresholding a matrix of activations at zero. Meanwhile, ReLUs
 does not suffer from saturating.</p>
</li>
<li><p>(<i>Pros</i>) It was found to greatly accelerate (e.g., a factor of 6 in <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">[1]</a>)
 the convergence of stochastic gradient descent compared to the 
sigmoid/tanh functions. It is argued that this is due to its linear, 
non-saturating form.</p>
</li>
<li><p>(<i>Cons</i>) Unfortunately, ReLU units can be fragile during 
training and can “die”. For example, a large gradient flowing through a 
ReLU neuron could cause the weights to update in such a way that the 
neuron will never activate on any datapoint again. If this happens, then
 the gradient flowing through the unit will forever be zero from that 
point on. That is, the ReLU units can irreversibly die during training 
since they can get knocked off the data manifold. For example, you may 
find that as much as 40% of your network can be “dead” (i.e., neurons 
that never activate across the entire training dataset) if the learning 
rate is set too high. With a proper setting of the learning rate this is
 less frequently an issue.</p>
</li>
</ol>
<h3>Leaky ReLU</h3>
<table class="imgtable"><tbody><tr><td>
<img src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/leaky.png" alt="lrelu" height="160px" width="240px">&nbsp;</td>
<td align="left"><p>Leaky ReLUs are one attempt to fix the “dying ReLU” problem. Instead of the function being zero when <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/5999452984636080433-130.png" alt="x&lt;0" style="vertical-align: -1px">, a leaky ReLU will instead have a small negative slope (of 0.01, or so). That is, the function computes <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/4196616167090173439-130.png" alt="f(x)=alpha x" style="vertical-align: -5px"> if <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/5999452984636080433-130.png" alt="x&lt;0" style="vertical-align: -1px"> and <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/5484684767813338902-130.png" alt="f(x)=x" style="vertical-align: -5px"> if <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/4437941819613442466-130.png" alt="xgeq 0" style="vertical-align: -3px">, where <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/7227645438722938594-130.png" alt="alpha" style="vertical-align: -1px">
 is a small constant. Some people report success with this form of 
activation function, but the results are not always consistent.</p>
</td></tr></tbody></table>
<h3>Parametric ReLU</h3>
<p>Nowadays, a broader class of activation functions, namely the <b><i>rectified unit family</i></b>, were proposed. In the following, we will talk about the variants of ReLU.</p>
<table class="imgtable"><tbody><tr><td>
<img src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/relufamily.png" alt="relufamily" height="160px" width="640px">&nbsp;</td>
<td align="left"><p>ReLU, Leaky ReLU, PReLU and RReLU. In these figures, for PReLU, <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/2031553203773749404-130.png" alt="alpha_i" style="vertical-align: -4px"> is learned and for Leaky ReLU <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/2031553203773749404-130.png" alt="alpha_i" style="vertical-align: -4px"> is fixed. For RReLU, <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/8090385205122200253-130.png" alt="alpha_{ji}" style="vertical-align: -6px"> is a random variable keeps sampling in a given range, and remains fixed in testing.</p>
</td></tr></tbody></table>
<p>The first variant is called <i>parametric rectified linear unit</i> (<i>PReLU</i>) <a href="http://arxiv.org/abs/1502.01852">[4]</a>. In PReLU, the slopes of negative part are learned from data rather than pre-defined. He <i>et al</i>. <a href="http://arxiv.org/abs/1502.01852">[4]</a> claimed that PReLU is the key factor of surpassing human-level performance on <a href="http://www.image-net.org/">ImageNet</a>
 classification task. The back-propagation and updating process of PReLU
 is very straightforward and similar to traditional ReLU, which is shown
 in Page. 43 of the slides.</p>
<h3>Randomized ReLU</h3>
<p>The second variant is called <i>randomized rectified linear unit</i> (<i>RReLU</i>).
 In RReLU, the slopes of negative parts are randomized in a given range 
in the training, and then fixed in the testing. As mentioned in <a href="http://arxiv.org/abs/1505.00853">[5]</a>, in a recent Kaggle <a href="https://www.kaggle.com/c/datasciencebowl">National Data Science Bowl (NDSB)</a>
 competition, it is reported that RReLU could reduce overfitting due to 
its randomized nature. Moreover, suggested by the NDSB competition 
winner, the random <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/2031553203773749404-130.png" alt="alpha_i" style="vertical-align: -4px"> in training is sampled from <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/348464217043394429-130.png" alt="1/U(3,8)" style="vertical-align: -5px"> and in test time it is fixed as its expectation, i.e., <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/6504549066650438132-130.png" alt="2/(l+u)=2/11" style="vertical-align: -5px">.</p>
<p>In <a href="http://arxiv.org/abs/1505.00853">[5]</a>, the authors 
evaluated classification performance of two state-of-the-art CNN 
architectures with different activation functions on the <i>CIFAR-10</i>, <i>CIFAR-100</i> and <i>NDSB</i> data sets, which are shown in the following tables. <i>Please note that, for these two networks, activation function is followed by each convolutional layer. And the <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/12416037344-130.png" alt="a" style="vertical-align: -1px"> in these tables actually indicates <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/6763807723309486172-130.png" alt="1/alpha" style="vertical-align: -5px">, where <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/7227645438722938594-130.png" alt="alpha" style="vertical-align: -1px"> is the aforementioned slopes</i>.</p>
<table class="imgtable"><tbody><tr><td>
<img src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/relures.png" alt="relures" height="160px" width="990px">&nbsp;</td>
<td align="left"></td></tr></tbody></table>
<p>From these tables, we can find the performance of ReLU is not the 
best for all the three data sets. For Leaky ReLU, a larger slope <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/7227645438722938594-130.png" alt="alpha" style="vertical-align: -1px">
 will achieve better accuracy rates. PReLU is easy to overfit on small 
data sets (its training error is the smallest, while testing error is 
not satisfactory), but still outperforms ReLU. In addition, RReLU is 
significantly better than other activation functions on NDSB, which 
shows RReLU can overcome overfitting, because this data set has less 
training data than that of CIFAR-10/CIFAR-100. <b><i>In conclusion, 
three types of ReLU variants all consistently outperform the original 
ReLU in these three data sets. And PReLU and RReLU seem better choices. 
Moreover, He </i>et al<i>. also reported similar conclusions in <a href="http://arxiv.org/abs/1502.01852">[4]</a></i></b>.</p>
<h2>Sec. 6: Regularizations</h2>
<p>There are several ways of controlling the capacity of Neural Networks to prevent overfitting:</p>
<ul>
<li><p><b>L2 regularization</b> is perhaps the most common form of 
regularization. It can be implemented by penalizing the squared 
magnitude of all parameters directly in the objective. That is, for 
every weight <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/15232045814-130.png" alt="w" style="vertical-align: -1px"> in the network, we add the term <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/4439456383146610173-130.png" alt="frac{1}{2}lambda w^2" style="vertical-align: -7px"> to the objective, where <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/9213219682577124982-130.png" alt="lambda" style="vertical-align: -1px"> is the regularization strength. It is common to see the factor of <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/7924204476109182584-130.png" alt="frac{1}{2}" style="vertical-align: -7px"> in front because then the gradient of this term with respect to the parameter <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/15232045814-130.png" alt="w" style="vertical-align: -1px"> is simply <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/2768874965512882837-130.png" alt="lambda w" style="vertical-align: -1px"> instead of <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/2017924880831122006-130.png" alt="2lambda w" style="vertical-align: -1px">.
 The L2 regularization has the intuitive interpretation of heavily 
penalizing peaky weight vectors and preferring diffuse weight vectors.</p>
</li>
<li><p><b>L1 regularization</b> is another relatively common form of regularization, where for each weight <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/15232045814-130.png" alt="w" style="vertical-align: -1px"> we add the term <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/1540651743016124243-130.png" alt="lambda |w|" style="vertical-align: -5px"> to the objective. It is possible to combine the L1 regularization with the L2 regularization: <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/5909288400939027289-130.png" alt="lambda_1 |w|+lambda_2 w^2" style="vertical-align: -5px"> (this is called <a href="http://web.stanford.edu/%7Ehastie/Papers/B67.2%20%282005%29%20301-320%20Zou%20&amp;%20Hastie.pdf">Elastic net regularization</a>).
 The L1 regularization has the intriguing property that it leads the 
weight vectors to become sparse during optimization (i.e. very close to 
exactly zero). In other words, neurons with L1 regularization end up 
using only a sparse subset of their most important inputs and become 
nearly invariant to the “noisy” inputs. In comparison, final weight 
vectors from L2 regularization are usually diffuse, small numbers. In 
practice, if you are not concerned with explicit feature selection, L2 
regularization can be expected to give superior performance over L1.</p>
</li>
<li><p><b>Max norm constraints</b>. Another form of regularization is to
 enforce an absolute upper bound on the magnitude of the weight vector 
for every neuron and use projected gradient descent to enforce the 
constraint. In practice, this corresponds to performing the parameter 
update as normal, and then enforcing the constraint by clamping the 
weight vector <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/6235765355760146286-130.png" alt="vec{w}" style="vertical-align: -1px"> of every neuron to satisfy <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/5858718376305099-130.png" alt="parallel vec{w} parallel_2 &lt;c" style="vertical-align: -5px">. Typical values of <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/12672038114-130.png" alt="c" style="vertical-align: -1px">
 are on orders of 3 or 4. Some people report improvements when using 
this form of regularization. One of its appealing properties is that 
network cannot “explode” even when the learning rates are set too high 
because the updates are always bounded.</p>
</li>
<li><p><b>Dropout</b> is an extremely effective, simple and recently introduced regularization technique by Srivastava <i>et al</i>. in <a href="http://jmlr.org/papers/v15/srivastava14a.html">[6]</a>
 that complements the other methods (L1, L2, maxnorm). During training, 
dropout can be interpreted as sampling a Neural Network within the full 
Neural Network, and only updating the parameters of the sampled network 
based on the input data. (However, the exponential number of possible 
sampled networks are not independent because they share the parameters.)
 During testing there is no dropout applied, with the interpretation of 
evaluating an averaged prediction across the exponentially-sized 
ensemble of all sub-networks (more about ensembles in the next section).
 In practice, the value of dropout ratio <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/7993929496961263-130.png" alt="p=0.5" style="vertical-align: -4px"> is a reasonable default, but this can be tuned on validation data.</p>
</li>
</ul>
<table class="imgtable"><tbody><tr><td>
<img src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/dropout.png" alt="dropout" height="160px" width="600px">&nbsp;</td>
<td align="left"><p>The most popular used regularization technique <i>dropout</i> <a href="http://jmlr.org/papers/v15/srivastava14a.html">[6]</a>. While training, dropout is implemented by only keeping a neuron active with some probability <img class="eq" src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/14336043121-130.png" alt="p" style="vertical-align: -4px"> (a hyper-parameter), or setting it to zero otherwise. In addition, Google applied for a <a href="https://www.google.com/patents/WO2014105866A1"><b>US patent</b></a> for <i>dropout</i> in 2014.</p>
</td></tr></tbody></table>
<h2>Sec. 7: Insights from Figures</h2>
<p>Finally, from the tips above, you can get the satisfactory settings 
(e.g., data processing, architectures choices and details, etc.) for 
your own deep networks. During training time, you can draw some figures 
to indicate your networks’ training effectiveness.</p>
<ul>
<li><p>As we have known, the learning rate is very sensitive. From Fig. 1
 in the following, a very high learning rate will cause a quite strange 
loss curve. A low learning rate will make your training loss decrease 
very slowly even after a large number of epochs. In contrast, a high 
learning rate will make training loss decrease fast at the beginning, 
but it will also drop into a local minimum. Thus, your networks might 
not achieve a satisfactory results in that case. For a good learning 
rate, as the red line shown in Fig. 1, its loss curve performs smoothly 
and finally it achieves the best performance.</p>
</li>
<li><p>Now let’s zoom in the loss curve. The epochs present the number 
of times for training once on the training data, so there are multiple 
mini batches in each epoch. If we draw the classification loss every 
training batch, the curve performs like Fig. 2. Similar to Fig. 1, if 
the trend of the loss curve looks too linear, that indicates your 
learning rate is low; if it does not decrease much, it tells you that 
the learning rate might be too high. Moreover, the “width” of the curve 
is related to the batch size. If the “width” looks too wide, that is to 
say the variance between every batch is too large, which points out you 
should increase the batch size.</p>
</li>
<li><p>Another tip comes from the accuracy curve. As shown in Fig. 3, 
the red line is the training accuracy, and the green line is the 
validation one. When the validation accuracy converges, the gap between 
the red line and the green one will show the effectiveness of your deep 
networks. If the gap is big, it indicates your network could get good 
accuracy on the training data, while it only achieve a low accuracy on 
the validation set. It is obvious that your deep model overfits on the 
training set. Thus, you should increase the regularization strength of 
deep networks. However, no gap meanwhile at a low accuracy level is not a
 good thing, which shows your deep model has low learnability. In that 
case, it is better to increase the model capacity for better results. </p>
</li>
</ul>
<table class="imgtable"><tbody><tr><td>
<img src="Must%20Know%20Tips_Tricks%20in%20Deep%20Neural%20Networks_files/trainfigs.png" alt="trainfigs" height="220px" width="1000px">&nbsp;</td>
<td align="left"></td></tr></tbody></table>
<h2>Sec. 8: Ensemble</h2>
<p>In machine learning, ensemble methods <a href="https://www.crcpress.com/Ensemble-Methods-Foundations-and-Algorithms/Zhou/9781439830031">[8]</a>
 that train multiple learners and then combine them for use are a kind 
of state-of-the-art learning approach. It is well known that an ensemble
 is usually significantly more accurate than a single learner, and 
ensemble methods have already achieved great success in many real-world 
tasks. In practical applications, especially challenges or competitions,
 almost all the first-place and second-place winners used ensemble 
methods.</p>
<p>Here we introduce several skills for ensemble in the deep learning scenario.</p>
<ul>
<li><p><b>Same model, different initialization</b>. Use cross-validation
 to determine the best hyperparameters, then train multiple models with 
the best set of hyperparameters but with different random 
initialization. The danger with this approach is that the variety is 
only due to initialization.</p>
</li>
<li><p><b>Top models discovered during cross-validation</b>. Use 
cross-validation to determine the best hyperparameters, then pick the 
top few (e.g., 10) models to form the ensemble. This improves the 
variety of the ensemble but has the danger of including suboptimal 
models. In practice, this can be easier to perform since it does not 
require additional retraining of models after cross-validation. 
Actually, you could directly select several state-of-the-art deep models
 from <a href="https://github.com/BVLC/caffe/wiki/Model-Zoo">Caffe Model Zoo</a> to perform ensemble.</p>
</li>
<li><p><b>Different checkpoints of a single model</b>. If training is 
very expensive, some people have had limited success in taking different
 checkpoints of a single network over time (for example after every 
epoch) and using those to form an ensemble. Clearly, this suffers from 
some lack of variety, but can still work reasonably well in practice. 
The advantage of this approach is that is very cheap.</p>
</li>
<li><p><b>Some practical examples</b>. If your vision tasks are related 
to high-level image semantic, e.g., event recognition from still images,
 a better ensemble method is to employ multiple deep models trained on 
different data sources to extract different and complementary deep 
representations. For example in the <a href="https://www.codalab.org/competitions/4081#learn_the_details">Cultural Event Recognition</a> challenge in associated with <a href="http://pamitc.org/iccv15/">ICCV’15</a>, we utilized five different deep models trained on images of <a href="http://www.image-net.org/">ImageNet</a>, <a href="http://places.csail.mit.edu/">Place Database</a> and the cultural images supplied by the <a href="http://gesture.chalearn.org/">competition organizers</a>.
 After that, we extracted five complementary deep features and treat 
them as multi-view data. Combining “early fusion” and “late fusion” 
strategies described in <a href="http://lamda.nju.edu.cn/weixs/publication/iccvw15_CER.pdf">[7]</a>, we achieved one of the best performance and ranked the 2nd place in that challenge. Similar to our work, <a href="http://cs231n.stanford.edu/reports/milad_final_report.pdf">[9]</a> presented the <i>Stacked NN</i> framework to fuse more deep networks at the same time.</p>
</li>
</ul>
<h2>Miscellaneous</h2>
<p>In real world applications, the data is usually <b>class-imbalanced</b>:
 some classes have a large number of images/training instances, while 
some have very limited number of images. As discussed in a recent 
technique report <a href="http://www.diva-portal.org/smash/get/diva2:811111/FULLTEXT01.pdf">[10]</a>,
 when deep CNNs are trained on these imbalanced training sets, the 
results show that imbalanced training data can potentially have a 
severely negative impact on overall performance in deep networks. For 
this issue, the simplest method is to balance the training data by 
directly up-sampling and down-sampling the imbalanced data, which is 
shown in <a href="http://www.diva-portal.org/smash/get/diva2:811111/FULLTEXT01.pdf">[10]</a>. Another interesting solution is one kind of special crops processing in our challenge solution <a href="http://lamda.nju.edu.cn/weixs/publication/iccvw15_CER.pdf">[7]</a>.
 Because the original cultural event images are imbalanced, we merely 
extract crops from the classes which have a small number of training 
images, which on one hand can supply diverse data sources, and on the 
other hand can solve the class-imbalanced problem. In addition, you can 
adjust the fine-tuning strategy for overcoming class-imbalance. For 
example, you can divide your own data set into two parts: one contains 
the classes which have a large number of training samples 
(images/crops); the other contains the classes of limited number of 
samples. In each part, the class-imbalanced problem will be not very 
serious. At the beginning of fine-tuning on your data set, you firstly 
fine-tune on the classes which have a large number of training samples 
(images/crops), and secondly, continue to fine-tune but on the classes 
with limited number samples.</p>
<h2>References &amp; Source Links</h2>
<ol>
<li><p>A. Krizhevsky, I. Sutskever, and G. E. Hinton. <a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">ImageNet Classification with Deep Convolutional Neural Networks</a>. In <i>NIPS</i>, 2012</p>
</li>
<li><p><a href="http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html/">A Brief Overview of Deep Learning</a>, which is a guest post by <a href="http://www.cs.toronto.edu/%7Eilya/">Ilya Sutskever</a>.</p>
</li>
<li><p><a href="http://cs231n.stanford.edu/index.html/">CS231n: Convolutional Neural Networks for Visual Recognition</a> of <i>Stanford University</i>, held by <a href="http://vision.stanford.edu/index.html">Prof. Fei-Fei Li</a> and <a href="http://cs.stanford.edu/people/karpathy/">Andrej Karpathy</a>.</p>
</li>
<li><p>K. He, X. Zhang, S. Ren, and J. Sun. <a href="http://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification</a>. In <i>ICCV</i>, 2015.</p>
</li>
<li><p>B. Xu, N. Wang, T. Chen, and M. Li. <a href="http://arxiv.org/abs/1505.00853">Empirical Evaluation of Rectified Activations in Convolution Network</a>. In <i>ICML Deep Learning Workshop</i>, 2015.</p>
</li>
<li><p>N. Srivastava, G. E. Hinton,  A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. <a href="http://jmlr.org/papers/v15/srivastava14a.html">Dropout: A Simple Way to Prevent Neural Networks from Overfitting</a>. <i>JMLR</i>, 15(Jun):1929−1958, 2014.</p>
</li>
<li><p>X.-S. Wei, B.-B. Gao, and J. Wu. <a href="http://lamda.nju.edu.cn/weixs/publication/iccvw15_CER.pdf">Deep Spatial Pyramid Ensemble for Cultural Event Recognition</a>. In <i>ICCV ChaLearn Looking at People Workshop</i>, 2015.
</p>
</li>
<li><p>Z.-H. Zhou. <a href="https://www.crcpress.com/Ensemble-Methods-Foundations-and-Algorithms/Zhou/9781439830031">Ensemble Methods: Foundations and Algorithms</a>. <i>Boca Raton, FL: Chapman &amp; Hall</i>CRC/, 2012. (ISBN 978-1-439-830031)</p>
</li>
<li><p>M. Mohammadi, and S. Das. <a href="http://cs231n.stanford.edu/reports/milad_final_report.pdf">S-NN: Stacked Neural Networks</a>. Project in <a href="http://cs231n.stanford.edu/reports.html"><i>Stanford CS231n Winter Quarter</i></a>, 2015.</p>
</li>
<li><p>P. Hensman, and D. Masko. <a href="http://www.diva-portal.org/smash/get/diva2:811111/FULLTEXT01.pdf">The Impact of Imbalanced Training Data for Convolutional Neural Networks</a>. <i>Degree Project in Computer Science</i>, DD143X, 2015.

</p>
</li>
</ol>
<div id="footer">
<div id="footer-text">
Copyright@2015, Xiu-Shen Wei. (Page generated 2015-10-19 23:28:37 CST.)</div>
</div>
</div>


</body></html>